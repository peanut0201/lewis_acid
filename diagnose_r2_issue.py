#!/usr/bin/env python3
"""
è¯Šæ–­RÂ²ä¸ºè´Ÿå€¼çš„é—®é¢˜
"""

import tensorflow as tf
import numpy as np
import yaml
import ast
import os
from sklearn.metrics import r2_score, mean_absolute_error

from dimenet.model.dimenet_pp import create_dimenet_pp_from_data_container
from dimenet.model.activations import swish
from dimenet.training.trainer import Trainer
from dimenet.training.metrics import Metrics
from dimenet.training.data_container import DataContainer
from dimenet.training.data_provider import DataProvider

def main():
    print("=== RÂ²é—®é¢˜è¯Šæ–­ ===")
    
    # åŠ è½½é…ç½®
    with open('config_pp.yaml', 'r') as c:
        config = yaml.safe_load(c)
    
    for key, val in config.items():
        if type(val) is str:
            try:
                config[key] = ast.literal_eval(val)
            except (ValueError, SyntaxError):
                pass
    
    # åˆ›å»ºæ•°æ®å®¹å™¨
    data_container = DataContainer(
        json_file='data/FIA49k_Al.json', 
        xlsx_file='data/FIA49k.xlsx', 
        cutoff=config['cutoff'], 
        target_keys=None
    )
    
    print(f"âœ… æ•°æ®å®¹å™¨åˆ›å»ºå®Œæˆ")
    print(f"   ç›®æ ‡é”®: {data_container.target_keys}")
    print(f"   æ•°æ®é‡: {len(data_container)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_dimenet_pp_from_data_container(
        data_container,
        emb_size=config['emb_size'], 
        out_emb_size=config['out_emb_size'],
        int_emb_size=config['int_emb_size'], 
        basis_emb_size=config['basis_emb_size'],
        num_blocks=config['num_blocks'], 
        num_spherical=config['num_spherical'], 
        num_radial=config['num_radial'],
        cutoff=config['cutoff'], 
        envelope_exponent=config['envelope_exponent'],
        num_before_skip=config['num_before_skip'], 
        num_after_skip=config['num_after_skip'],
        num_dense_output=config['num_dense_output'],
        activation=swish, 
        extensive=config['extensive'], 
        output_init=config['output_init'],
        freeze_backbone=config['freeze_backbone']
    )
    
    # æ„å»ºæ¨¡å‹
    data_provider = DataProvider(
        data_container, 
        config['num_train'], 
        config['num_valid'], 
        config['batch_size'],
        seed=config['data_seed'], 
        randomized=True
    )
    train_dataset = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)
    train_dataset_iter = iter(train_dataset)
    inputs, targets = next(train_dataset_iter)
    _ = model(inputs)  # æ„å»ºæ¨¡å‹
    
    print("âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model, 
        config['learning_rate'], 
        config['warmup_steps'],
        config['decay_steps'], 
        config['decay_rate'],
        ema_decay=config['ema_decay'], 
        max_grad_norm=1000,
        freeze_backbone=config['freeze_backbone']
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
    best_ckpt_file = 'logs_20250922_234938/best_ckpt'
    if os.path.exists(best_ckpt_file + '.index'):
        print(f"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {best_ckpt_file}")
        model.load_weights(best_ckpt_file)
        print("âœ… æœ€ä½³æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
    else:
        print("âŒ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æƒé‡æ–‡ä»¶")
        return
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\n=== æµ‹è¯•é›†è¯„ä¼° ===")
    test_dataset = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)
    test_dataset_iter = iter(test_dataset)
    
    all_predictions = []
    all_targets = []
    
    num_test = data_provider.nsamples['test']
    num_batches = int(np.ceil(num_test / config['batch_size']))
    
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {num_test}")
    print(f"è¯„ä¼°æ‰¹æ¬¡æ•°: {num_batches}")
    
    for i in range(num_batches):
        try:
            inputs, targets = next(test_dataset_iter)
            preds = model(inputs, training=False)
            
            all_predictions.append(preds.numpy())
            all_targets.append(targets.numpy())
            
            if (i + 1) % 5 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{num_batches} æ‰¹æ¬¡")
                
        except StopIteration:
            break
        except Exception as e:
            print(f"æµ‹è¯•æ‰¹æ¬¡ {i} å‡ºé”™: {e}")
            break
    
    if all_predictions and all_targets:
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)
        
        print(f"\n=== é¢„æµ‹ç»“æœåˆ†æ ===")
        print(f"é¢„æµ‹å€¼å½¢çŠ¶: {all_predictions.shape}")
        print(f"çœŸå®å€¼å½¢çŠ¶: {all_targets.shape}")
        
        print(f"\né¢„æµ‹å€¼ç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{all_predictions.min():.4f}, {all_predictions.max():.4f}]")
        print(f"  å‡å€¼: {all_predictions.mean(axis=0)}")
        print(f"  æ ‡å‡†å·®: {all_predictions.std(axis=0)}")
        
        print(f"\nçœŸå®å€¼ç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{all_targets.min():.4f}, {all_targets.max():.4f}]")
        print(f"  å‡å€¼: {all_targets.mean(axis=0)}")
        print(f"  æ ‡å‡†å·®: {all_targets.std(axis=0)}")
        
        print(f"\n=== å„ç›®æ ‡è¯¦ç»†åˆ†æ ===")
        target_keys = data_container.target_keys
        
        for i, target_key in enumerate(target_keys):
            pred_i = all_predictions[:, i]
            true_i = all_targets[:, i]
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            r2 = r2_score(true_i, pred_i)
            mae = mean_absolute_error(true_i, pred_i)
            mse = np.mean((pred_i - true_i) ** 2)
            
            # è®¡ç®—åŸºå‡†çº¿ï¼ˆå‡å€¼é¢„æµ‹ï¼‰
            mean_pred = np.full_like(true_i, true_i.mean())
            r2_baseline = r2_score(true_i, mean_pred)
            mae_baseline = mean_absolute_error(true_i, mean_pred)
            
            print(f"\n{target_key}:")
            print(f"  RÂ²: {r2:.4f}")
            print(f"  MAE: {mae:.4f}")
            print(f"  MSE: {mse:.4f}")
            print(f"  é¢„æµ‹å€¼èŒƒå›´: [{pred_i.min():.4f}, {pred_i.max():.4f}]")
            print(f"  çœŸå®å€¼èŒƒå›´: [{true_i.min():.4f}, {true_i.max():.4f}]")
            print(f"  é¢„æµ‹å€¼å‡å€¼: {pred_i.mean():.4f}")
            print(f"  çœŸå®å€¼å‡å€¼: {true_i.mean():.4f}")
            print(f"  åŸºå‡†çº¿RÂ² (å‡å€¼é¢„æµ‹): {r2_baseline:.4f}")
            print(f"  åŸºå‡†çº¿MAE (å‡å€¼é¢„æµ‹): {mae_baseline:.4f}")
            
            # åˆ†æé—®é¢˜
            if r2 < 0:
                print(f"  âŒ RÂ²ä¸ºè´Ÿå€¼ï¼æ¨¡å‹é¢„æµ‹æ¯”å‡å€¼é¢„æµ‹è¿˜å·®")
                print(f"  ğŸ’¡ å¯èƒ½åŸå› :")
                print(f"     - æ¨¡å‹é¢„æµ‹å€¼èŒƒå›´å¼‚å¸¸")
                print(f"     - é¢„æµ‹å€¼ä¸çœŸå®å€¼ç›¸å…³æ€§æå·®")
                print(f"     - æ¨¡å‹æƒé‡åŠ è½½æœ‰é—®é¢˜")
            else:
                print(f"  âœ… RÂ²æ­£å¸¸")
        
        # æ•´ä½“åˆ†æ
        print(f"\n=== æ•´ä½“åˆ†æ ===")
        overall_r2 = r2_score(all_targets.flatten(), all_predictions.flatten())
        overall_mae = mean_absolute_error(all_targets.flatten(), all_predictions.flatten())
        
        print(f"æ•´ä½“RÂ²: {overall_r2:.4f}")
        print(f"æ•´ä½“MAE: {overall_mae:.4f}")
        
        if overall_r2 < 0:
            print(f"\nâŒ æ•´ä½“RÂ²ä¸ºè´Ÿå€¼ï¼")
            print(f"ğŸ’¡ å¯èƒ½çš„é—®é¢˜:")
            print(f"   1. æ¨¡å‹æƒé‡åŠ è½½é”™è¯¯")
            print(f"   2. æ•°æ®é¢„å¤„ç†é—®é¢˜")
            print(f"   3. æ¨¡å‹æ¶æ„ä¸åŒ¹é…")
            print(f"   4. è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é—®é¢˜")
            
            # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦å¼‚å¸¸
            if np.allclose(all_predictions, 0):
                print(f"   ğŸ” é¢„æµ‹å€¼å…¨ä¸º0ï¼Œå¯èƒ½æ˜¯æƒé‡åŠ è½½é—®é¢˜")
            elif np.std(all_predictions) < 0.01:
                print(f"   ğŸ” é¢„æµ‹å€¼æ–¹å·®æå°ï¼Œå¯èƒ½æ˜¯æ¨¡å‹é€€åŒ–")
            elif np.abs(all_predictions.mean()) > 10:
                print(f"   ğŸ” é¢„æµ‹å€¼å‡å€¼å¼‚å¸¸å¤§ï¼Œå¯èƒ½æ˜¯æ•°å€¼ä¸ç¨³å®š")
        else:
            print(f"âœ… æ•´ä½“RÂ²æ­£å¸¸")

if __name__ == "__main__":
    main()
