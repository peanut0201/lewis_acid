{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet_pp import create_dimenet_pp_from_data_container\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置日志系统\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "# 设置TensorFlow日志级别\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置加载完成！\n",
      "模型: dimenet++\n",
      "冻结主干: True\n",
      "预训练路径: pretrained/dimenet_pp/alpha\n",
      "训练步数: 50000\n",
      "批次大小: 16\n"
     ]
    }
   ],
   "source": [
    "# 加载配置文件\n",
    "with open('config_pp.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "\n",
    "# 冻结训练参数\n",
    "freeze_backbone = config.get('freeze_backbone', False)\n",
    "pretrained_model_path = config.get('pretrained_model_path', None)\n",
    "\n",
    "# 解析字符串配置（处理None等特殊值）\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "# 提取DimeNet++模型配置参数\n",
    "model_name = config['model_name']\n",
    "if model_name != \"dimenet++\":\n",
    "    raise ValueError(f\"只支持DimeNet++模型，当前配置: '{model_name}'\")\n",
    "\n",
    "out_emb_size = config['out_emb_size']\n",
    "int_emb_size = config['int_emb_size']\n",
    "basis_emb_size = config['basis_emb_size']\n",
    "extensive = config['extensive']\n",
    "    \n",
    "# 提取通用配置参数\n",
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "output_init = config['output_init']\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "# 提取训练配置参数\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']\n",
    "\n",
    "print(f\"配置加载完成！\")\n",
    "print(f\"模型: {model_name}\")\n",
    "print(f\"冻结主干: {freeze_backbone}\")\n",
    "print(f\"预训练路径: {pretrained_model_path}\")\n",
    "print(f\"训练步数: {num_steps}\")\n",
    "print(f\"批次大小: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取 2215 个化合物的FIA目标值\n",
      "目标值形状: (2215, 4)\n",
      "FIA列: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "目标值标准化完成 - 均值: [469.7716  461.22308 282.7777  274.22876], 标准差: [57.100338 61.692844 47.78473  52.19486 ]\n",
      "数据容器加载完成！\n",
      "目标键: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "数据量: 2215\n",
      "目标值形状: (2215, 4)\n"
     ]
    }
   ],
   "source": [
    "# 加载FIA数据集\n",
    "json_file = 'data/FIA49k_Al.json'\n",
    "xlsx_file = 'data/FIA49k.xlsx'\n",
    "\n",
    "# 创建数据容器\n",
    "data_container = DataContainer(\n",
    "    json_file=json_file, \n",
    "    xlsx_file=xlsx_file, \n",
    "    cutoff=cutoff, \n",
    "    target_keys=None  # 自动从Excel文件确定目标键\n",
    ")\n",
    "\n",
    "print(f\"数据容器加载完成！\")\n",
    "print(f\"目标键: {data_container.target_keys}\")\n",
    "print(f\"数据量: {len(data_container)}\")\n",
    "print(f\"目标值形状: {data_container.targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 数据集划分 ===\n",
      "总样本数: 2215\n",
      "训练集: 1600 样本 (72.2%)\n",
      "验证集: 400 样本 (18.1%)\n",
      "测试集: 215 样本 (9.7%)\n",
      "数据划分正常\n"
     ]
    }
   ],
   "source": [
    "# 显示数据集划分情况\n",
    "print(\"=== 数据集划分 ===\")\n",
    "total_samples = len(data_container)\n",
    "num_test = total_samples - num_train - num_valid\n",
    "\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "print(f\"训练集: {num_train} 样本 ({num_train/total_samples*100:.1f}%)\")\n",
    "print(f\"验证集: {num_valid} 样本 ({num_valid/total_samples*100:.1f}%)\")\n",
    "print(f\"测试集: {num_test} 样本 ({num_test/total_samples*100:.1f}%)\")\n",
    "\n",
    "# 验证数据划分\n",
    "if num_test <= 0:\n",
    "    print(\"警告: 测试集样本数为0或负数, 请检查数据划分参数\")\n",
    "else:\n",
    "    print(\"数据划分正常\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建DimeNet++模型 ===\n",
      "根据数据容器创建模型，目标数量: 4\n",
      "目标键: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "主干网络已冻结，只训练输出头\n",
      "主干网络已冻结，只训练输出头\n",
      "模型创建完成！目标数量: 4\n",
      "构建模型...\n",
      "FIA目标值形状: [None, 4]\n",
      "FIA目标键: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "目标值数量: 4\n",
      "FIA目标值形状: (16, 4)\n",
      "FIA目标值示例: [[-0.07125839  0.03458285  0.03160409  0.14777401]\n",
      " [ 0.02963555  0.09814543 -0.43191    -0.3118239 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 09:40:27.810409: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/opt/anaconda3/envs/lewis_acid_env/lib/python3.9/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型构建完成！\n",
      "模型参数总数: 1889670\n",
      "=== 可训练层信息 ===\n",
      "  emb_block: 已冻结\n",
      "  rbf_layer: 已冻结\n",
      "  sbf_layer: 已冻结\n",
      "  int_block_0: 已冻结\n",
      "  int_block_1: 已冻结\n",
      "  int_block_2: 已冻结\n",
      "  int_block_3: 已冻结\n",
      "  output_block_0: 可训练, 参数数量: 231936\n",
      "  output_block_1: 可训练, 参数数量: 231936\n",
      "  output_block_2: 可训练, 参数数量: 231936\n",
      "  output_block_3: 可训练, 参数数量: 231936\n",
      "  output_block_4: 可训练, 参数数量: 231936\n",
      "   可训练参数数量: 1159680\n"
     ]
    }
   ],
   "source": [
    "# 创建DimeNet++模型\n",
    "print(\"=== 创建DimeNet++模型 ===\")\n",
    "model = create_dimenet_pp_from_data_container(\n",
    "    data_container,\n",
    "    emb_size=emb_size, out_emb_size=out_emb_size,\n",
    "    int_emb_size=int_emb_size, basis_emb_size=basis_emb_size,\n",
    "    num_blocks=num_blocks, num_spherical=num_spherical, num_radial=num_radial,\n",
    "    cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "    num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "    num_dense_output=num_dense_output,\n",
    "    activation=swish, extensive=extensive, output_init=output_init,\n",
    "    freeze_backbone=freeze_backbone)\n",
    "\n",
    "print(f\"模型创建完成！目标数量: {model.num_targets}\")\n",
    "\n",
    "# 构建模型（通过前向传播）\n",
    "print(\"构建模型...\")\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                           seed=data_seed, randomized=True)\n",
    "train_dataset = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "inputs, targets = next(train_dataset_iter)\n",
    "_ = model(inputs)  # 这会构建模型\n",
    "\n",
    "print(f\"模型构建完成！\")\n",
    "print(f\"模型参数总数: {model.count_params()}\")\n",
    "\n",
    "# 显示可训练层信息（在模型构建后）\n",
    "if hasattr(model, 'print_trainable_layers'):\n",
    "    model.print_trainable_layers()\n",
    "print(f\"   可训练参数数量: {model.get_trainable_params_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 加载预训练权重 ===\n",
      "正在加载预训练权重: pretrained/dimenet_pp/alpha\n",
      "尝试加载兼容的预训练权重...\n",
      "✅ 预训练权重部分加载成功，跳过了不兼容的输出层\n",
      "   可训练参数数量: 1159680\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练权重\n",
    "if pretrained_model_path and freeze_backbone:\n",
    "    print(\"=== 加载预训练权重 ===\")\n",
    "    print(f\"正在加载预训练权重: {pretrained_model_path}\")\n",
    "    \n",
    "    # 检查预训练模型是否存在\n",
    "    if os.path.exists(pretrained_model_path):\n",
    "        try:\n",
    "            # 尝试加载兼容的权重，跳过不兼容的输出层\n",
    "            print(\"尝试加载兼容的预训练权重...\")\n",
    "            checkpoint_path = os.path.join(pretrained_model_path, 'ckpt')\n",
    "            if os.path.exists(checkpoint_path + '.index'):\n",
    "                # 使用tf.train.Checkpoint只加载兼容的变量\n",
    "                ckpt = tf.train.Checkpoint(model=model)\n",
    "                # 只恢复兼容的变量，跳过输出层\n",
    "                status = ckpt.restore(checkpoint_path)\n",
    "                # 检查哪些变量被恢复了\n",
    "                restored_vars = status.expect_partial()\n",
    "                print(f\"预训练权重部分加载成功，跳过了不兼容的输出层\")\n",
    "            else:\n",
    "                print(\"未找到checkpoint文件, 跳过预训练权重加载\")\n",
    "            \n",
    "            # 显示可训练参数数量\n",
    "            if hasattr(model, 'get_trainable_params_count'):\n",
    "                trainable_params = model.get_trainable_params_count()\n",
    "                print(f\"可训练参数数量: {trainable_params}\")\n",
    "        except Exception as e:\n",
    "            print(f\"预训练权重加载失败: {e}\")\n",
    "            print(\"将使用随机初始化的权重\")\n",
    "    else:\n",
    "        print(f\"预训练模型路径不存在: {pretrained_model_path}\")\n",
    "        print(\"将使用随机初始化的权重\")\n",
    "elif freeze_backbone:\n",
    "    print(\"=== 冻结主干网络 ===\")\n",
    "    if hasattr(model, 'get_trainable_params_count'):\n",
    "        trainable_params = model.get_trainable_params_count()\n",
    "        print(f\"   可训练参数数量: {trainable_params}\")\n",
    "else:\n",
    "    print(\"=== 正常训练模式 ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建训练器 ===\n",
      "✅ 训练器初始化完成！\n",
      "   冻结主干: True\n",
      "   学习率: 0.001\n",
      "   优化器: Adam\n"
     ]
    }
   ],
   "source": [
    "# 创建训练器\n",
    "print(\"=== 创建训练器 ===\")\n",
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000,\n",
    "                  freeze_backbone=freeze_backbone)\n",
    "\n",
    "print(f\"✅ 训练器初始化完成！\")\n",
    "print(f\"   冻结主干: {freeze_backbone}\")\n",
    "print(f\"   学习率: {learning_rate}\")\n",
    "print(f\"   优化器: {type(trainer.optimizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集评估函数\n",
    "def evaluate_on_test_set(model, data_provider, batch_size, trainer, max_batches=10):\n",
    "    \"\"\"在测试集上评估模型\"\"\"\n",
    "    test_dataset = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset_iter = iter(test_dataset)\n",
    "    test_metrics = Metrics('test', data_container.target_keys)\n",
    "    \n",
    "    num_test = len(data_container) - num_train - num_valid\n",
    "    num_batches = min(int(np.ceil(num_test / batch_size)), max_batches)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            inputs, targets = next(test_dataset_iter)\n",
    "            preds = model(inputs, training=False)\n",
    "            loss, mae = trainer.compute_loss(targets, preds)\n",
    "            nsamples = tf.shape(preds)[0]\n",
    "            test_metrics.update_state(loss, loss, mae, nsamples)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"测试批次 {i} 出错: {e}\")\n",
    "            break\n",
    "    \n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学习率调度器: LinearWarmupExponentialDecay\n",
      "基础学习率: 0.001\n",
      "冻结训练: True\n"
     ]
    }
   ],
   "source": [
    "# 学习率已经在Trainer中根据freeze_backbone自动调整\n",
    "print(f\"学习率调度器: {type(trainer.optimizer.learning_rate).__name__}\")\n",
    "print(f\"基础学习率: {learning_rate}\")\n",
    "print(f\"冻结训练: {freeze_backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建训练和验证数据集 ===\n",
      "✅ 数据集创建完成！\n",
      "   训练集: 1600 样本\n",
      "   验证集: 400 样本\n",
      "✅ 日志目录创建完成: logs_20250922_204908\n",
      "🔍 测试数据集迭代器...\n",
      "❌ 训练数据集迭代器出错: 'node_attr'\n",
      "🔍 测试模型前向传播...\n",
      "✅ 模型前向传播正常 - 预测形状: (16, 4)\n"
     ]
    }
   ],
   "source": [
    "# 创建训练和验证数据集\n",
    "print(\"=== 创建训练和验证数据集 ===\")\n",
    "\n",
    "# 创建训练和验证字典\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "# 创建训练数据集\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "\n",
    "# 创建验证数据集\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])\n",
    "\n",
    "# 创建训练和验证指标\n",
    "train['metrics'] = Metrics('train', data_container.target_keys)\n",
    "validation['metrics'] = Metrics('val', data_container.target_keys)\n",
    "\n",
    "print(f\"✅ 数据集创建完成！\")\n",
    "print(f\"   训练集: {num_train} 样本\")\n",
    "print(f\"   验证集: {num_valid} 样本\")\n",
    "\n",
    "# 创建日志目录\n",
    "log_dir = f\"logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "step_ckpt_folder = log_dir\n",
    "best_loss_file = os.path.join(log_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(log_dir, 'best_ckpt')\n",
    "\n",
    "# 初始化最佳指标\n",
    "metrics_best = {k: np.inf for k in validation['metrics'].keys()}\n",
    "metrics_best['step'] = 0\n",
    "\n",
    "# 创建summary writer\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "print(f\"✅ 日志目录创建完成: {log_dir}\")\n",
    "\n",
    "# 测试数据集迭代器和模型\n",
    "print(\"🔍 测试数据集迭代器...\")\n",
    "try:\n",
    "    test_inputs, test_targets = next(train['dataset_iter'])\n",
    "    print(f\"✅ 训练数据集迭代器正常 - 输入形状: {test_inputs['node_attr'].shape}, 目标形状: {test_targets.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练数据集迭代器出错: {e}\")\n",
    "\n",
    "print(\"🔍 测试模型前向传播...\")\n",
    "try:\n",
    "    test_preds = model(test_inputs, training=True)\n",
    "    print(f\"✅ 模型前向传播正常 - 预测形状: {test_preds.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型前向传播出错: {e}\")\n",
    "\n",
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Note that the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` is expected. It is due to the backward pass of `tf.gather` producing sparse gradients, which the previous layer has to convert to a dense tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始训练！总步数: 50000, 每轮: 100步\n",
      "==================================================\n",
      "🆕 从头开始训练\n",
      "🔍 步骤 1: 开始训练...\n",
      "✅ 步骤 1: 训练完成，损失 = 239.933670\n",
      "🔍 步骤 2: 开始训练...\n",
      "✅ 步骤 2: 训练完成，损失 = 180.053955\n",
      "🔍 步骤 3: 开始训练...\n",
      "✅ 步骤 3: 训练完成，损失 = 196.125381\n",
      "🔍 步骤 4: 开始训练...\n",
      "✅ 步骤 4: 训练完成，损失 = 183.205048\n",
      "🔍 步骤 5: 开始训练...\n",
      "✅ 步骤 5: 训练完成，损失 = 253.412033\n",
      "🔍 步骤 10: 开始训练...\n",
      "✅ 步骤 10: 训练完成，损失 = 158.679871\n",
      "🔍 步骤 20: 开始训练...\n",
      "✅ 步骤 20: 训练完成，损失 = 145.188889\n",
      "🔍 步骤 30: 开始训练...\n",
      "✅ 步骤 30: 训练完成，损失 = 147.169403\n",
      "🔍 步骤 40: 开始训练...\n",
      "✅ 步骤 40: 训练完成，损失 = 86.645569\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "    \n",
    "    print(f\"🚀 开始训练！总步数: {num_steps}, 每轮: {steps_per_epoch}步\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "        print(f\"📂 从检查点恢复: 步骤 {step_init}\")\n",
    "    else:\n",
    "        step_init = 1\n",
    "        print(f\"🆕 从头开始训练\")\n",
    "    \n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # 添加调试输出\n",
    "        if step <= 5 or step % 10 == 0:\n",
    "            print(f\"🔍 步骤 {step}: 开始训练...\")\n",
    "        \n",
    "        try:\n",
    "            # Perform training step\n",
    "            loss = trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "            \n",
    "            if step <= 5 or step % 10 == 0:\n",
    "                print(f\"✅ 步骤 {step}: 训练完成，损失 = {loss:.6f}\")\n",
    "                \n",
    "        except StopIteration:\n",
    "            print(f\"⚠️ 步骤 {step}: 数据集迭代器耗尽，重新创建...\")\n",
    "            train['dataset_iter'] = iter(train['dataset'])\n",
    "            loss = trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 步骤 {step}: 训练出错 - {e}\")\n",
    "            break\n",
    "\n",
    "        # 每100步显示训练进度\n",
    "        if step % 100 == 0:\n",
    "            epoch = step // steps_per_epoch\n",
    "            progress = (step - step_init) / (num_steps - step_init + 1) * 100\n",
    "            print(f\"📊 步骤 {step}/{num_steps} (轮次 {epoch+1}) - 进度: {progress:.1f}% - 损失: {train['metrics'].loss:.6f}\")\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "            print(f\"💾 检查点已保存 (步骤 {step})\")\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "            print(f\"\\n🔍 评估模型 (步骤 {step})...\")\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "                print(f\"🏆 新的最佳模型已保存! 验证MAE: {validation['metrics'].mean_mae:.6f}\")\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "\n",
    "            epoch = step // steps_per_epoch\n",
    "            print(f\"📈 步骤 {step}/{num_steps} (轮次 {epoch+1}): 训练损失: {train['metrics'].loss:.6f}, 验证损失: {validation['metrics'].loss:.6f}\")\n",
    "            print(f\"   训练MAE: {train['metrics'].mean_mae:.6f}, 验证MAE: {validation['metrics'].mean_mae:.6f}, 最佳验证MAE: {metrics_best['mean_mae_val']:.6f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()\n",
    "    \n",
    "    print(f\"\\n🎉 训练完成! 最佳验证MAE: {metrics_best['mean_mae_val']:.6f}, 模型保存在: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完成后在测试集上评估模型\n",
    "print(\"=== 测试集评估 ===\")\n",
    "\n",
    "# 加载最佳模型权重\n",
    "if os.path.exists(best_ckpt_file):\n",
    "    print(f\"加载最佳模型权重: {best_ckpt_file}\")\n",
    "    model.load_weights(best_ckpt_file)\n",
    "else:\n",
    "    print(\"未找到最佳模型权重，使用当前模型\")\n",
    "\n",
    "# 在测试集上评估\n",
    "print(\"在测试集上评估模型...\")\n",
    "test_metrics = evaluate_on_test_set(model, data_provider, batch_size, trainer, max_batches=50)\n",
    "\n",
    "# 显示测试结果\n",
    "print(f\"\\n📊 测试集评估结果:\")\n",
    "print(f\"   测试损失: {test_metrics.loss:.6f}\")\n",
    "print(f\"   测试MAE: {test_metrics.mean_mae:.6f}\")\n",
    "\n",
    "# 显示每个目标的详细结果\n",
    "print(f\"\\n📈 各目标详细结果:\")\n",
    "for i, target_key in enumerate(data_container.target_keys):\n",
    "    target_mae = test_metrics.mae[i] if hasattr(test_metrics, 'mae') else \"N/A\"\n",
    "    print(f\"   {target_key}: MAE = {target_mae}\")\n",
    "\n",
    "# 保存测试结果\n",
    "test_results = {\n",
    "    'test_loss': test_metrics.loss,\n",
    "    'test_mae': test_metrics.mean_mae,\n",
    "    'target_keys': data_container.target_keys\n",
    "}\n",
    "np.savez(os.path.join(log_dir, 'test_results.npz'), **test_results)\n",
    "print(f\"\\n💾 测试结果已保存到: {os.path.join(log_dir, 'test_results.npz')}\")\n",
    "\n",
    "print(\"\\n🎯 训练和测试完成！所有结果已保存到日志目录。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewis_acid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
