{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet_pp import create_dimenet_pp_from_data_container\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®æ—¥å¿—ç³»ç»Ÿ\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®åŠ è½½å®Œæˆï¼\n",
      "æ¨¡å‹: dimenet++\n",
      "å†»ç»“ä¸»å¹²: True\n",
      "é¢„è®­ç»ƒè·¯å¾„: pretrained/dimenet_pp/alpha\n",
      "è®­ç»ƒæ­¥æ•°: 50000\n",
      "æ‰¹æ¬¡å¤§å°: 16\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½é…ç½®æ–‡ä»¶\n",
    "with open('config_pp.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "\n",
    "# å†»ç»“è®­ç»ƒå‚æ•°\n",
    "freeze_backbone = config.get('freeze_backbone', False)\n",
    "pretrained_model_path = config.get('pretrained_model_path', None)\n",
    "\n",
    "# è§£æå­—ç¬¦ä¸²é…ç½®ï¼ˆå¤„ç†Noneç­‰ç‰¹æ®Šå€¼ï¼‰\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "# æå–DimeNet++æ¨¡å‹é…ç½®å‚æ•°\n",
    "model_name = config['model_name']\n",
    "if model_name != \"dimenet++\":\n",
    "    raise ValueError(f\"åªæ”¯æŒDimeNet++æ¨¡å‹ï¼Œå½“å‰é…ç½®: '{model_name}'\")\n",
    "\n",
    "out_emb_size = config['out_emb_size']\n",
    "int_emb_size = config['int_emb_size']\n",
    "basis_emb_size = config['basis_emb_size']\n",
    "extensive = config['extensive']\n",
    "    \n",
    "# æå–é€šç”¨é…ç½®å‚æ•°\n",
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "output_init = config['output_init']\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "# æå–è®­ç»ƒé…ç½®å‚æ•°\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']\n",
    "\n",
    "print(f\"é…ç½®åŠ è½½å®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹: {model_name}\")\n",
    "print(f\"å†»ç»“ä¸»å¹²: {freeze_backbone}\")\n",
    "print(f\"é¢„è®­ç»ƒè·¯å¾„: {pretrained_model_path}\")\n",
    "print(f\"è®­ç»ƒæ­¥æ•°: {num_steps}\")\n",
    "print(f\"æ‰¹æ¬¡å¤§å°: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸæå– 2215 ä¸ªåŒ–åˆç‰©çš„FIAç›®æ ‡å€¼\n",
      "ç›®æ ‡å€¼å½¢çŠ¶: (2215, 4)\n",
      "FIAåˆ—: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "ç›®æ ‡å€¼æ ‡å‡†åŒ–å®Œæˆ - å‡å€¼: [469.7716  461.22308 282.7777  274.22876], æ ‡å‡†å·®: [57.100338 61.692844 47.78473  52.19486 ]\n",
      "æ•°æ®å®¹å™¨åŠ è½½å®Œæˆï¼\n",
      "ç›®æ ‡é”®: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "æ•°æ®é‡: 2215\n",
      "ç›®æ ‡å€¼å½¢çŠ¶: (2215, 4)\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½FIAæ•°æ®é›†\n",
    "json_file = 'data/FIA49k_Al.json'\n",
    "xlsx_file = 'data/FIA49k.xlsx'\n",
    "\n",
    "# åˆ›å»ºæ•°æ®å®¹å™¨\n",
    "data_container = DataContainer(\n",
    "    json_file=json_file, \n",
    "    xlsx_file=xlsx_file, \n",
    "    cutoff=cutoff, \n",
    "    target_keys=None  # è‡ªåŠ¨ä»Excelæ–‡ä»¶ç¡®å®šç›®æ ‡é”®\n",
    ")\n",
    "\n",
    "print(f\"æ•°æ®å®¹å™¨åŠ è½½å®Œæˆï¼\")\n",
    "print(f\"ç›®æ ‡é”®: {data_container.target_keys}\")\n",
    "print(f\"æ•°æ®é‡: {len(data_container)}\")\n",
    "print(f\"ç›®æ ‡å€¼å½¢çŠ¶: {data_container.targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ•°æ®é›†åˆ’åˆ† ===\n",
      "æ€»æ ·æœ¬æ•°: 2215\n",
      "è®­ç»ƒé›†: 1600 æ ·æœ¬ (72.2%)\n",
      "éªŒè¯é›†: 400 æ ·æœ¬ (18.1%)\n",
      "æµ‹è¯•é›†: 215 æ ·æœ¬ (9.7%)\n",
      "æ•°æ®åˆ’åˆ†æ­£å¸¸\n"
     ]
    }
   ],
   "source": [
    "# æ˜¾ç¤ºæ•°æ®é›†åˆ’åˆ†æƒ…å†µ\n",
    "print(\"=== æ•°æ®é›†åˆ’åˆ† ===\")\n",
    "total_samples = len(data_container)\n",
    "num_test = total_samples - num_train - num_valid\n",
    "\n",
    "print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "print(f\"è®­ç»ƒé›†: {num_train} æ ·æœ¬ ({num_train/total_samples*100:.1f}%)\")\n",
    "print(f\"éªŒè¯é›†: {num_valid} æ ·æœ¬ ({num_valid/total_samples*100:.1f}%)\")\n",
    "print(f\"æµ‹è¯•é›†: {num_test} æ ·æœ¬ ({num_test/total_samples*100:.1f}%)\")\n",
    "\n",
    "# éªŒè¯æ•°æ®åˆ’åˆ†\n",
    "if num_test <= 0:\n",
    "    print(\"è­¦å‘Š: æµ‹è¯•é›†æ ·æœ¬æ•°ä¸º0æˆ–è´Ÿæ•°, è¯·æ£€æŸ¥æ•°æ®åˆ’åˆ†å‚æ•°\")\n",
    "else:\n",
    "    print(\"æ•°æ®åˆ’åˆ†æ­£å¸¸\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åˆ›å»ºDimeNet++æ¨¡å‹ ===\n",
      "æ ¹æ®æ•°æ®å®¹å™¨åˆ›å»ºæ¨¡å‹ï¼Œç›®æ ‡æ•°é‡: 4\n",
      "ç›®æ ‡é”®: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "ä¸»å¹²ç½‘ç»œå·²å†»ç»“ï¼Œåªè®­ç»ƒè¾“å‡ºå¤´\n",
      "ä¸»å¹²ç½‘ç»œå·²å†»ç»“ï¼Œåªè®­ç»ƒè¾“å‡ºå¤´\n",
      "æ¨¡å‹åˆ›å»ºå®Œæˆï¼ç›®æ ‡æ•°é‡: 4\n",
      "æ„å»ºæ¨¡å‹...\n",
      "FIAç›®æ ‡å€¼å½¢çŠ¶: [None, 4]\n",
      "FIAç›®æ ‡é”®: ['fia_gas-DSDBLYP', 'fia_gas-PBEh3c', 'fia_solv-DSDBLYP', 'fia_solv-PBEh3c']\n",
      "ç›®æ ‡å€¼æ•°é‡: 4\n",
      "FIAç›®æ ‡å€¼å½¢çŠ¶: (16, 4)\n",
      "FIAç›®æ ‡å€¼ç¤ºä¾‹: [[-0.07125839  0.03458285  0.03160409  0.14777401]\n",
      " [ 0.02963555  0.09814543 -0.43191    -0.3118239 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 09:40:27.810409: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/opt/anaconda3/envs/lewis_acid_env/lib/python3.9/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹æ„å»ºå®Œæˆï¼\n",
      "æ¨¡å‹å‚æ•°æ€»æ•°: 1889670\n",
      "=== å¯è®­ç»ƒå±‚ä¿¡æ¯ ===\n",
      "  emb_block: å·²å†»ç»“\n",
      "  rbf_layer: å·²å†»ç»“\n",
      "  sbf_layer: å·²å†»ç»“\n",
      "  int_block_0: å·²å†»ç»“\n",
      "  int_block_1: å·²å†»ç»“\n",
      "  int_block_2: å·²å†»ç»“\n",
      "  int_block_3: å·²å†»ç»“\n",
      "  output_block_0: å¯è®­ç»ƒ, å‚æ•°æ•°é‡: 231936\n",
      "  output_block_1: å¯è®­ç»ƒ, å‚æ•°æ•°é‡: 231936\n",
      "  output_block_2: å¯è®­ç»ƒ, å‚æ•°æ•°é‡: 231936\n",
      "  output_block_3: å¯è®­ç»ƒ, å‚æ•°æ•°é‡: 231936\n",
      "  output_block_4: å¯è®­ç»ƒ, å‚æ•°æ•°é‡: 231936\n",
      "   å¯è®­ç»ƒå‚æ•°æ•°é‡: 1159680\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºDimeNet++æ¨¡å‹\n",
    "print(\"=== åˆ›å»ºDimeNet++æ¨¡å‹ ===\")\n",
    "model = create_dimenet_pp_from_data_container(\n",
    "    data_container,\n",
    "    emb_size=emb_size, out_emb_size=out_emb_size,\n",
    "    int_emb_size=int_emb_size, basis_emb_size=basis_emb_size,\n",
    "    num_blocks=num_blocks, num_spherical=num_spherical, num_radial=num_radial,\n",
    "    cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "    num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "    num_dense_output=num_dense_output,\n",
    "    activation=swish, extensive=extensive, output_init=output_init,\n",
    "    freeze_backbone=freeze_backbone)\n",
    "\n",
    "print(f\"æ¨¡å‹åˆ›å»ºå®Œæˆï¼ç›®æ ‡æ•°é‡: {model.num_targets}\")\n",
    "\n",
    "# æ„å»ºæ¨¡å‹ï¼ˆé€šè¿‡å‰å‘ä¼ æ’­ï¼‰\n",
    "print(\"æ„å»ºæ¨¡å‹...\")\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                           seed=data_seed, randomized=True)\n",
    "train_dataset = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "inputs, targets = next(train_dataset_iter)\n",
    "_ = model(inputs)  # è¿™ä¼šæ„å»ºæ¨¡å‹\n",
    "\n",
    "print(f\"æ¨¡å‹æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹å‚æ•°æ€»æ•°: {model.count_params()}\")\n",
    "\n",
    "# æ˜¾ç¤ºå¯è®­ç»ƒå±‚ä¿¡æ¯ï¼ˆåœ¨æ¨¡å‹æ„å»ºåï¼‰\n",
    "if hasattr(model, 'print_trainable_layers'):\n",
    "    model.print_trainable_layers()\n",
    "print(f\"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {model.get_trainable_params_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åŠ è½½é¢„è®­ç»ƒæƒé‡ ===\n",
      "æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡: pretrained/dimenet_pp/alpha\n",
      "å°è¯•åŠ è½½å…¼å®¹çš„é¢„è®­ç»ƒæƒé‡...\n",
      "âœ… é¢„è®­ç»ƒæƒé‡éƒ¨åˆ†åŠ è½½æˆåŠŸï¼Œè·³è¿‡äº†ä¸å…¼å®¹çš„è¾“å‡ºå±‚\n",
      "   å¯è®­ç»ƒå‚æ•°æ•°é‡: 1159680\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½é¢„è®­ç»ƒæƒé‡\n",
    "if pretrained_model_path and freeze_backbone:\n",
    "    print(\"=== åŠ è½½é¢„è®­ç»ƒæƒé‡ ===\")\n",
    "    print(f\"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_model_path}\")\n",
    "    \n",
    "    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "    if os.path.exists(pretrained_model_path):\n",
    "        try:\n",
    "            # å°è¯•åŠ è½½å…¼å®¹çš„æƒé‡ï¼Œè·³è¿‡ä¸å…¼å®¹çš„è¾“å‡ºå±‚\n",
    "            print(\"å°è¯•åŠ è½½å…¼å®¹çš„é¢„è®­ç»ƒæƒé‡...\")\n",
    "            checkpoint_path = os.path.join(pretrained_model_path, 'ckpt')\n",
    "            if os.path.exists(checkpoint_path + '.index'):\n",
    "                # ä½¿ç”¨tf.train.CheckpointåªåŠ è½½å…¼å®¹çš„å˜é‡\n",
    "                ckpt = tf.train.Checkpoint(model=model)\n",
    "                # åªæ¢å¤å…¼å®¹çš„å˜é‡ï¼Œè·³è¿‡è¾“å‡ºå±‚\n",
    "                status = ckpt.restore(checkpoint_path)\n",
    "                # æ£€æŸ¥å“ªäº›å˜é‡è¢«æ¢å¤äº†\n",
    "                restored_vars = status.expect_partial()\n",
    "                print(f\"é¢„è®­ç»ƒæƒé‡éƒ¨åˆ†åŠ è½½æˆåŠŸï¼Œè·³è¿‡äº†ä¸å…¼å®¹çš„è¾“å‡ºå±‚\")\n",
    "            else:\n",
    "                print(\"æœªæ‰¾åˆ°checkpointæ–‡ä»¶, è·³è¿‡é¢„è®­ç»ƒæƒé‡åŠ è½½\")\n",
    "            \n",
    "            # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°æ•°é‡\n",
    "            if hasattr(model, 'get_trainable_params_count'):\n",
    "                trainable_params = model.get_trainable_params_count()\n",
    "                print(f\"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params}\")\n",
    "        except Exception as e:\n",
    "            print(f\"é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}\")\n",
    "            print(\"å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡\")\n",
    "    else:\n",
    "        print(f\"é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {pretrained_model_path}\")\n",
    "        print(\"å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡\")\n",
    "elif freeze_backbone:\n",
    "    print(\"=== å†»ç»“ä¸»å¹²ç½‘ç»œ ===\")\n",
    "    if hasattr(model, 'get_trainable_params_count'):\n",
    "        trainable_params = model.get_trainable_params_count()\n",
    "        print(f\"   å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params}\")\n",
    "else:\n",
    "    print(\"=== æ­£å¸¸è®­ç»ƒæ¨¡å¼ ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åˆ›å»ºè®­ç»ƒå™¨ ===\n",
      "âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\n",
      "   å†»ç»“ä¸»å¹²: True\n",
      "   å­¦ä¹ ç‡: 0.001\n",
      "   ä¼˜åŒ–å™¨: Adam\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒå™¨\n",
    "print(\"=== åˆ›å»ºè®­ç»ƒå™¨ ===\")\n",
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000,\n",
    "                  freeze_backbone=freeze_backbone)\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "print(f\"   å†»ç»“ä¸»å¹²: {freeze_backbone}\")\n",
    "print(f\"   å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"   ä¼˜åŒ–å™¨: {type(trainer.optimizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•é›†è¯„ä¼°å‡½æ•°\n",
    "def evaluate_on_test_set(model, data_provider, batch_size, trainer, max_batches=10):\n",
    "    \"\"\"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\"\"\"\n",
    "    test_dataset = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset_iter = iter(test_dataset)\n",
    "    test_metrics = Metrics('test', data_container.target_keys)\n",
    "    \n",
    "    num_test = len(data_container) - num_train - num_valid\n",
    "    num_batches = min(int(np.ceil(num_test / batch_size)), max_batches)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            inputs, targets = next(test_dataset_iter)\n",
    "            preds = model(inputs, training=False)\n",
    "            loss, mae = trainer.compute_loss(targets, preds)\n",
    "            nsamples = tf.shape(preds)[0]\n",
    "            test_metrics.update_state(loss, loss, mae, nsamples)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"æµ‹è¯•æ‰¹æ¬¡ {i} å‡ºé”™: {e}\")\n",
    "            break\n",
    "    \n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å­¦ä¹ ç‡è°ƒåº¦å™¨: LinearWarmupExponentialDecay\n",
      "åŸºç¡€å­¦ä¹ ç‡: 0.001\n",
      "å†»ç»“è®­ç»ƒ: True\n"
     ]
    }
   ],
   "source": [
    "# å­¦ä¹ ç‡å·²ç»åœ¨Trainerä¸­æ ¹æ®freeze_backboneè‡ªåŠ¨è°ƒæ•´\n",
    "print(f\"å­¦ä¹ ç‡è°ƒåº¦å™¨: {type(trainer.optimizer.learning_rate).__name__}\")\n",
    "print(f\"åŸºç¡€å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"å†»ç»“è®­ç»ƒ: {freeze_backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›† ===\n",
      "âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼\n",
      "   è®­ç»ƒé›†: 1600 æ ·æœ¬\n",
      "   éªŒè¯é›†: 400 æ ·æœ¬\n",
      "âœ… æ—¥å¿—ç›®å½•åˆ›å»ºå®Œæˆ: logs_20250922_204908\n",
      "ğŸ” æµ‹è¯•æ•°æ®é›†è¿­ä»£å™¨...\n",
      "âŒ è®­ç»ƒæ•°æ®é›†è¿­ä»£å™¨å‡ºé”™: 'node_attr'\n",
      "ğŸ” æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...\n",
      "âœ… æ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸ - é¢„æµ‹å½¢çŠ¶: (16, 4)\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†\n",
    "print(\"=== åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›† ===\")\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯å­—å…¸\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒæ•°æ®é›†\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "\n",
    "# åˆ›å»ºéªŒè¯æ•°æ®é›†\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡\n",
    "train['metrics'] = Metrics('train', data_container.target_keys)\n",
    "validation['metrics'] = Metrics('val', data_container.target_keys)\n",
    "\n",
    "print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼\")\n",
    "print(f\"   è®­ç»ƒé›†: {num_train} æ ·æœ¬\")\n",
    "print(f\"   éªŒè¯é›†: {num_valid} æ ·æœ¬\")\n",
    "\n",
    "# åˆ›å»ºæ—¥å¿—ç›®å½•\n",
    "log_dir = f\"logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "step_ckpt_folder = log_dir\n",
    "best_loss_file = os.path.join(log_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(log_dir, 'best_ckpt')\n",
    "\n",
    "# åˆå§‹åŒ–æœ€ä½³æŒ‡æ ‡\n",
    "metrics_best = {k: np.inf for k in validation['metrics'].keys()}\n",
    "metrics_best['step'] = 0\n",
    "\n",
    "# åˆ›å»ºsummary writer\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "print(f\"âœ… æ—¥å¿—ç›®å½•åˆ›å»ºå®Œæˆ: {log_dir}\")\n",
    "\n",
    "# æµ‹è¯•æ•°æ®é›†è¿­ä»£å™¨å’Œæ¨¡å‹\n",
    "print(\"ğŸ” æµ‹è¯•æ•°æ®é›†è¿­ä»£å™¨...\")\n",
    "try:\n",
    "    test_inputs, test_targets = next(train['dataset_iter'])\n",
    "    print(f\"âœ… è®­ç»ƒæ•°æ®é›†è¿­ä»£å™¨æ­£å¸¸ - è¾“å…¥å½¢çŠ¶: {test_inputs['node_attr'].shape}, ç›®æ ‡å½¢çŠ¶: {test_targets.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®­ç»ƒæ•°æ®é›†è¿­ä»£å™¨å‡ºé”™: {e}\")\n",
    "\n",
    "print(\"ğŸ” æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...\")\n",
    "try:\n",
    "    test_preds = model(test_inputs, training=True)\n",
    "    print(f\"âœ… æ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸ - é¢„æµ‹å½¢çŠ¶: {test_preds.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹å‰å‘ä¼ æ’­å‡ºé”™: {e}\")\n",
    "\n",
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Note that the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` is expected. It is due to the backward pass of `tf.gather` producing sparse gradients, which the previous layer has to convert to a dense tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒï¼æ€»æ­¥æ•°: 50000, æ¯è½®: 100æ­¥\n",
      "==================================================\n",
      "ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ\n",
      "ğŸ” æ­¥éª¤ 1: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 1: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 239.933670\n",
      "ğŸ” æ­¥éª¤ 2: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 2: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 180.053955\n",
      "ğŸ” æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 3: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 196.125381\n",
      "ğŸ” æ­¥éª¤ 4: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 4: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 183.205048\n",
      "ğŸ” æ­¥éª¤ 5: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 5: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 253.412033\n",
      "ğŸ” æ­¥éª¤ 10: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 10: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 158.679871\n",
      "ğŸ” æ­¥éª¤ 20: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 20: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 145.188889\n",
      "ğŸ” æ­¥éª¤ 30: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 30: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 147.169403\n",
      "ğŸ” æ­¥éª¤ 40: å¼€å§‹è®­ç»ƒ...\n",
      "âœ… æ­¥éª¤ 40: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = 86.645569\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹è®­ç»ƒï¼æ€»æ­¥æ•°: {num_steps}, æ¯è½®: {steps_per_epoch}æ­¥\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "        print(f\"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤: æ­¥éª¤ {step_init}\")\n",
    "    else:\n",
    "        step_init = 1\n",
    "        print(f\"ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ\")\n",
    "    \n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # æ·»åŠ è°ƒè¯•è¾“å‡º\n",
    "        if step <= 5 or step % 10 == 0:\n",
    "            print(f\"ğŸ” æ­¥éª¤ {step}: å¼€å§‹è®­ç»ƒ...\")\n",
    "        \n",
    "        try:\n",
    "            # Perform training step\n",
    "            loss = trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "            \n",
    "            if step <= 5 or step % 10 == 0:\n",
    "                print(f\"âœ… æ­¥éª¤ {step}: è®­ç»ƒå®Œæˆï¼ŒæŸå¤± = {loss:.6f}\")\n",
    "                \n",
    "        except StopIteration:\n",
    "            print(f\"âš ï¸ æ­¥éª¤ {step}: æ•°æ®é›†è¿­ä»£å™¨è€—å°½ï¼Œé‡æ–°åˆ›å»º...\")\n",
    "            train['dataset_iter'] = iter(train['dataset'])\n",
    "            loss = trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ­¥éª¤ {step}: è®­ç»ƒå‡ºé”™ - {e}\")\n",
    "            break\n",
    "\n",
    "        # æ¯100æ­¥æ˜¾ç¤ºè®­ç»ƒè¿›åº¦\n",
    "        if step % 100 == 0:\n",
    "            epoch = step // steps_per_epoch\n",
    "            progress = (step - step_init) / (num_steps - step_init + 1) * 100\n",
    "            print(f\"ğŸ“Š æ­¥éª¤ {step}/{num_steps} (è½®æ¬¡ {epoch+1}) - è¿›åº¦: {progress:.1f}% - æŸå¤±: {train['metrics'].loss:.6f}\")\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "            print(f\"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜ (æ­¥éª¤ {step})\")\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "            print(f\"\\nğŸ” è¯„ä¼°æ¨¡å‹ (æ­¥éª¤ {step})...\")\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "                print(f\"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! éªŒè¯MAE: {validation['metrics'].mean_mae:.6f}\")\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "\n",
    "            epoch = step // steps_per_epoch\n",
    "            print(f\"ğŸ“ˆ æ­¥éª¤ {step}/{num_steps} (è½®æ¬¡ {epoch+1}): è®­ç»ƒæŸå¤±: {train['metrics'].loss:.6f}, éªŒè¯æŸå¤±: {validation['metrics'].loss:.6f}\")\n",
    "            print(f\"   è®­ç»ƒMAE: {train['metrics'].mean_mae:.6f}, éªŒè¯MAE: {validation['metrics'].mean_mae:.6f}, æœ€ä½³éªŒè¯MAE: {metrics_best['mean_mae_val']:.6f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯MAE: {metrics_best['mean_mae_val']:.6f}, æ¨¡å‹ä¿å­˜åœ¨: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå®Œæˆååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n",
    "print(\"=== æµ‹è¯•é›†è¯„ä¼° ===\")\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\n",
    "if os.path.exists(best_ckpt_file):\n",
    "    print(f\"åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {best_ckpt_file}\")\n",
    "    model.load_weights(best_ckpt_file)\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹\")\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "print(\"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...\")\n",
    "test_metrics = evaluate_on_test_set(model, data_provider, batch_size, trainer, max_batches=50)\n",
    "\n",
    "# æ˜¾ç¤ºæµ‹è¯•ç»“æœ\n",
    "print(f\"\\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:\")\n",
    "print(f\"   æµ‹è¯•æŸå¤±: {test_metrics.loss:.6f}\")\n",
    "print(f\"   æµ‹è¯•MAE: {test_metrics.mean_mae:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ¯ä¸ªç›®æ ‡çš„è¯¦ç»†ç»“æœ\n",
    "print(f\"\\nğŸ“ˆ å„ç›®æ ‡è¯¦ç»†ç»“æœ:\")\n",
    "for i, target_key in enumerate(data_container.target_keys):\n",
    "    target_mae = test_metrics.mae[i] if hasattr(test_metrics, 'mae') else \"N/A\"\n",
    "    print(f\"   {target_key}: MAE = {target_mae}\")\n",
    "\n",
    "# ä¿å­˜æµ‹è¯•ç»“æœ\n",
    "test_results = {\n",
    "    'test_loss': test_metrics.loss,\n",
    "    'test_mae': test_metrics.mean_mae,\n",
    "    'target_keys': data_container.target_keys\n",
    "}\n",
    "np.savez(os.path.join(log_dir, 'test_results.npz'), **test_results)\n",
    "print(f\"\\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {os.path.join(log_dir, 'test_results.npz')}\")\n",
    "\n",
    "print(\"\\nğŸ¯ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°æ—¥å¿—ç›®å½•ã€‚\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewis_acid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
